# ─────────────────────────────────────────────────
# LM Studio Configuration
# ─────────────────────────────────────────────────
LMSTUDIO_BASE_URL=http://127.0.0.1:1234/v1
LMSTUDIO_API_KEY=lm-studio
LMSTUDIO_MODEL=your-model-id
LMSTUDIO_JUDGE_MODEL=your-model-id
LLM_TIMEOUT_MS=120000

# ─────────────────────────────────────────────────
# Telemetry
# ─────────────────────────────────────────────────
TELEMETRY_REPORT_INTERVAL_MS=60000
TELEMETRY_LOG_REQUESTS=true
TELEMETRY_INCLUDE_LLM_OUTPUT=false
TELEMETRY_LLM_PREVIEW_CHARS=800

# ─────────────────────────────────────────────────
# Generation Settings
# ─────────────────────────────────────────────────
GEN_TEMPERATURE=0.7
GEN_MAX_TOKENS=4096

# ─────────────────────────────────────────────────
# Distributional Evaluation (Paper Section 3.1)
# ─────────────────────────────────────────────────
# Number of replicate runs per epic to capture output distribution
EVAL_REPLICATES=5

# Base seed for reproducibility; each replicate uses seedBase + i
EVAL_SEED_BASE=12345

# K for "discoverability within K tries" approximation
DISCOVERABILITY_TRIES=3

# ─────────────────────────────────────────────────
# Robust Objective Tuning
# ─────────────────────────────────────────────────
# Penalty coefficient for score variance (higher = favor consistent prompts)
EVAL_STD_LAMBDA=0.25

# Penalty for low pass rate (higher = favor reliable prompts)
EVAL_FAIL_PENALTY=0.4

# ─────────────────────────────────────────────────
# Contrastive Pair Mining (Paper Section 3.2)
# ─────────────────────────────────────────────────
# Minimum cosine similarity for outputs to be "near neighbors"
PAIR_MIN_SIM=0.86

# Minimum score delta for a pair to be "contrastive"
PAIR_MIN_DELTA=0.15

# Maximum contrastive pairs to feed to the patch engineer
PAIR_MAX_PAIRS=8

# ─────────────────────────────────────────────────
# Prompt Patch Optimization (Paper Section 3.3)
# ─────────────────────────────────────────────────
# Number of patch candidates to generate per optimization round
OPT_PATCH_CANDIDATES=10

# Temperature for patch engineer LLM (higher = more diverse patches)
OPT_PATCH_TEMPERATURE=0.6

# ─────────────────────────────────────────────────
# Optimizer Settings
# ─────────────────────────────────────────────────
# Number of optimization iterations
OPT_ITERATIONS=10

# Improvement threshold for promotion (avoids noise-based promotions)
OPT_PROMOTION_THRESHOLD=0.01

# Concurrency for parallel epic evaluation
OPT_CONCURRENCY=2

# ─────────────────────────────────────────────────
# Deno Deploy Configuration (Production)
# ─────────────────────────────────────────────────
# These are REQUIRED when deploying to Deno Deploy.
# Set them in: Deno Deploy → Project → Settings → Environment Variables
#
# LLM API endpoint (OpenRouter, OpenAI, etc.)
LLM_BASE_URL=https://openrouter.ai/api/v1
LLM_API_KEY=your-api-key
LLM_MODEL=openai/gpt-4o-mini

# ─────────────────────────────────────────────────
# Legacy: Ollama Cloud proxy (deprecated)
# ─────────────────────────────────────────────────
OLLAMA_API_KEY=your-ollama-cloud-api-key
OLLAMA_MODEL=your-model-id
OLLAMA_API_BASE_URL=https://ollama.com/api
